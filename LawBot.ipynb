{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Law Chatbot Using Retrieval Augmented Generation (RAG)\n",
        "\n",
        "This project aims to develop a Large Language Model (LLM) with Retrieval Augmented Generation (RAG) support.\n",
        "\n",
        "Specifically, we'd like to be able to open a PDF file, ask questions (queries) of it and have them answered by a Large Language Model (LLM)."
      ],
      "metadata": {
        "id": "LmYEZMXhIqmD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "RAG stands for Retrieval Augmented Generation.\n",
        "\n",
        "It was introduced in the paper [*Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks*](https://arxiv.org/abs/2005.11401).\n",
        "\n",
        "Each step can be roughly broken down to:\n",
        "\n",
        "* **Retrieval** - Seeking relevant information from a source given a query. For example, getting relevant passages of Wikipedia text from a database given a question.\n",
        "* **Augmented** - Using the relevant retrieved information to modify an input to a generative model (e.g. an LLM).\n",
        "* **Generation** - Generating an output given an input. For example, in the case of an LLM, generating a passage of text given an input prompt."
      ],
      "metadata": {
        "id": "vF5lSmR7IvCx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Why RAG?"
      ],
      "metadata": {
        "id": "NfFqqAgQIy1v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The primary goal of Retrieval-Augmented Generation (RAG) is to enhance the output quality of large language models (LLMs).\n",
        "\n",
        "Two primary improvements can be seen as:\n",
        "1. **Preventing hallucinations** - LLMs are powerful but can sometimes generate plausible yet incorrect information. RAG pipelines mitigate this by providing factual inputs, leading to more accurate outputs. Even if the answer from a RAG pipeline is questionable, the retrieval process provides access to the original sources, ensuring transparency.\n",
        "2. **Work with custom data** - While base LLMs excel at general language tasks due to their broad training on internet-scale text, they often lack specific domain knowledge. RAG systems address this by supplying LLMs with specialized data, such as medical records or corporate documentation, tailoring their outputs to specific applications."
      ],
      "metadata": {
        "id": "5kI8z9cPI0y8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting Up Our Workspace"
      ],
      "metadata": {
        "id": "CNwQiGHWI5rB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fetching Necessary Libraries**"
      ],
      "metadata": {
        "id": "bCbqMrDvI9ct"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "if \"COLAB_GPU\" in os.environ:\n",
        "    print(\" Running in Google Colab, installing requirements.\")\n",
        "    !pip install -U torch # requires torch 2.1.1+ (for efficient sdpa implementation)\n",
        "    !pip install PyMuPDF # for reading PDFs with Python\n",
        "    !pip install tqdm # for progress bars\n",
        "    !pip install sentence-transformers # for embedding models\n",
        "    !pip install accelerate # for quantization model loading\n",
        "    !pip install bitsandbytes # for quantizing models (less storage space)\n",
        "    !pip install flash-attn --no-build-isolation # for faster attention mechanism = faster LLM inference"
      ],
      "metadata": {
        "id": "cCi4GoXjIrMY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}